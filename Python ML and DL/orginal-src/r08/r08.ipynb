{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 [Sebastian Raschka](sebastianraschka.com)\n",
    "\n",
    "https://github.com/rasbt/python-machine-learning-book-3rd-edition\n",
    "\n",
    "[MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python. Uczenie maszynowe - kod źródłowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Applying Machine Learning To Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że rozszerzenie zawierające nieobowiązkowy znak wodny stanowi niewielki plugin notatnika IPython / Jupyter, który zaprojektowałem w celu powielania kodu źródłowego. Wystarczy pominąć poniższe wiersze kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "last updated: 2019-12-05 \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.10.1\n",
      "\n",
      "numpy 1.17.4\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "nltk 3.4.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Sebastian Raschka\" -u -d -v -p numpy,pandas,sklearn,nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Korzystanie z rozszerzenia `watermark` nie jest obowiązkowe. Możesz je zainstalować za pomocą polecenia:*  \n",
    "\n",
    "    conda install watermark -c conda-forge  \n",
    "\n",
    "lub  \n",
    "\n",
    "    pip install watermark   \n",
    "\n",
    "*Więcej informacji znajdziesz pod adresem: https://github.com/rasbt/watermark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spis treści"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Przygotowywanie zestawu danych IMDb movie review do przetwarzania tekstu](#Przygotowywanie-zestawu-danych-IMDb-movie-review-do-przetwarzania-tekstu)\n",
    "  - [Uzyskiwanie zestawu danych IMDb](#Uzyskiwanie-zestawu-danych-IMDb)\n",
    "  - [Przetwarzanie wstępne zestawu danych IMDb do wygodniejszego formatu](#Przetwarzanie-wstępne-zestawu-danych-IMDb-do-wygodniejszego-formatu)\n",
    "- [Wprowadzenie do modelu worka słów](#Wprowadzenie-do-modelu-worka-słów)\n",
    "  - [Przekształcanie słów w wektory cech](#Przekształcanie-słów-w-wektory-cech)\n",
    "  - [Ocena istotności wyrazów za pomocą ważenia częstości termów — odwrotnej częstości w tekście](#Ocena-istotności-wyrazów-za-pomocą-ważenia-częstości-termów-—-odwrotnej-częstości-w-tekście)\n",
    "  - [Oczyszczanie danych tekstowych](#Oczyszczanie-danych-tekstowych)\n",
    "  - [Przetwarzanie tekstu na znaczniki](#Przetwarzanie-tekstu-na-znaczniki)\n",
    "- [Uczenie modelu regresji logistycznej w celu klasyfikowania tekstu](#Uczenie-modelu-regresji-logistycznej-w-celu-klasyfikowania-tekstu)\n",
    "- [Praca z większą ilością danych — algorytmy sieciowe i uczenie pozardzeniowe](#Praca-z-większą-ilością-danych-—-algorytmy-sieciowe-i-uczenie-pozardzeniowe)\n",
    "- [Modelowanie tematyczne za pomocą alokacji ukrytej zmiennej Dirichleta](#Modelowanie-tematyczne-za-pomocą-alokacji-ukrytej-zmiennej-Dirichleta)\n",
    "  - [Rozkładanie dokumentów tekstowych za pomocą analizy LDA](#Rozkładanie-dokumentów-tekstowych-za-pomocą-analizy-LDA)\n",
    "  - [Analiza LDA w bibliotece scikit-learn](#Analiza-LDA-w-bibliotece-scikit-learn)\n",
    "- [Podsumowanie](#Podsumowanie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowywanie zestawu danych IMDb movie review do przetwarzania tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uzyskiwanie zestawu danych IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zestaw danych IMDB movie review jest dostępny do pobrania pod adresem [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "Po pobraniu pliku należy go rozpakować.\n",
    "\n",
    "A) Jeżeli używasz systemu Linux lub MacOS X, otwórz nowe okno terminala i za pomocą polecenia `cd` przejdź do katalogu, w którym znajduje się pobrane archiwum, a następnie wpisz komendę: \n",
    "\n",
    "`tar -zxf aclImdb_v1.tar.gz`\n",
    "\n",
    "B) W przypadku systemu Windows pobierz archiwizator (np. [7Zip](http://www.7-zip.org)), dzięki któremu otworzysz skompresowany plik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatywny kod służący do pobrania i rozpakowania zestawu danych w środowisku Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32% | 25 MB | 6.34 MB/s | 4 sec elapsed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72% | 58 MB | 7.86 MB/s | 7 sec elapsed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 80 MB | 8.24 MB/s | 9 sec elapsed"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024.**2 * duration)\n",
    "    percent = count * block_size * 100. / total_size\n",
    "\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d s szacowanego czasu\" %\n",
    "                    (percent, progress_size / (1024.**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "    urllib.request.urlretrieve(source, target, reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('aclImdb'):\n",
    "\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie wstępne zestawu danych IMDb do wygodniejszego formatu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:51\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# zmień wartość zmiennej `basepath` na ścieżkę do katalogu, w którym\n",
    "# znajduje się nierozpakowany plik z zestawem danych\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['Recenzja', 'Sentyment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasowanie zawartości obiektu DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcjonalnie: Zapisywanie zgromadzonych danych w postaci pliku CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Uwaga\n",
    "\n",
    "Jeżeli masz problem z utworzeniem pliku `movie_data.csv`, możesz pobrać jego zarchiwizowaną postać ze strony \n",
    "https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch08/.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie do modelu worka słów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przekształcanie słów w wektory cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołując metodę fit_transform w CountVectorizer stworzyliśmy wokabularz modelu worka słów i przekształciliśmy trzy następujące zdanie w rzadkie wektory cech:\n",
    "\n",
    "1. Słońce grzeje dziś mocno\n",
    "2. Pogoda jest dziś wiosenna\n",
    "3. Słońce grzeje dziś mocno i pogoda jest dziś wiosenna, a jeden i jeden daje dwa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'Słońce grzeje dziś mocno',\n",
    "        'Pogoda jest dziś wiosenna',\n",
    "        'Słońce grzeje dziś mocno i pogoda jest dziś wiosenna, a jeden i jeden daje dwa'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy teraz zawartość tego wokabularza, aby lepiej zrozumieć pojęcia, jakimi będziemy się posługiwać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać powyżej, wokabularz jest przechowywany w słowniku Pythona, w którym poszczególne wyrazy otrzymują indeksy liczbowe. Spójrzmy na wygenerowane przez nas wektory cech:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozycja każdego indeksu w ukazanych wektorach cech odpowiada liczbom całkowitym przechowywanym jako elementy słownika CountVectorizer; np. pierwsza cecha o indeksie 0 reprezentuje zliczenia wyrazu „daje”, który pojawia się wyłącznie w ostatnim zdaniu, natomiast słowo „dziś” na trzeciej pozycji (indeks 2 w wektorach cech) występuje we wszystkich trzech zdaniach. Wartości przechowywane w wektorach cech są również nazywane częstością termów: *tf (t, d)* — liczbą wystąpień termu t w dokumencie *d*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena istotności wyrazów za pomocą ważenia częstości termów — odwrotnej częstości w tekście"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W czasie analizowania danych tekstowych często natrafiamy na wyrazy z obydwu klas pojawiające się w wielu różnych danych tekstowych. Takie często występujące słowa zazwyczaj nie zawierają żadnych przydatnych ani rozróżniających informacji. W tym ustępie poznamy użyteczną technikę zwaną ważeniem częstości termów — odwrotną częstością w tekście (tf-idf), za pomocą której zmniejszamy wagi takich mniej istotnych wyrazów w wektorach cech. Parametr tf-idf możemy zdefiniować jako iloczyn częstości termów przez odwrotną częstość w tekście:\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "Tutaj tf (t, d) jest wprowadzoną w poprzednim ustępie częstością termów, natomiast odwrotną częstość w tekście idf (t, d) defi-niujemy następująco:\n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
    "\n",
    "gdzie $n_d$ oznacza całkowitą liczbę danych tekstowych, a *df(d, t)* — liczbę danych tekstowych *d* zawierających term *t*. Zwróć uwagę, że wstawienie stałej 1 do mianownika nie jest konieczne i służy do przydzielania niezerowej wartości termom znajdującym się we wszystkich próbkach uczących; logarytm gwarantuje, że niewielkie częstości danych tekstowych nie będą otrzymywały zbyt dużej wagi.\n",
    "\n",
    "Biblioteka scikit-learn zawiera jeszcze jedną klasę transformującą, `TfidfTransformer`, przyjmującą częstości termów przechowywane w klasie `CountVectorizer` i przekształcające je w wartości tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak wiemy z poprzedniego ustępu, wyraz „dziś” ma największą częstość termu w trzecim dokumencie, przez co stanowi najczęściej występujące słowo. Jednak po przekształceniu tego samego wektora cech za pomocą algorytmu tf-idf widzimy, że słowo to jest powiązane teraz ze względnie niską wartością tf-idf (0,36) w trzecim dokumencie, ponieważ występuje również w pozostałych dwóch dokumentach tekstowych, zatem jest mało prawdopodobne, że jest ono nośnikiem przydatnych, rozróżniających informacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdybyśmy jednak ręcznie policzyli wartości tf-idf poszczególnych termów w wektorach cech, okazałoby się, że klasa `TfidfTransformer` oblicza je nieco inaczej niż standardowe, omówione wcześniej wzory. Równanie na odwrotną częstość tekstów zaimplementowane w bibliotece scikit-learn wygląda następująco:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "Z kolei wzór na model tf-idf używany w bibliotece scikit-learn wygląda następująco:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "Zazwyczaj częstości termów są normalizowane jeszcze przed obliczeniem wartości tf-idf, ale klasa `TfidfTransformer` normalizuje wyniki tf-idf bezpośrednio.\n",
    "\n",
    "Domyślnie (`norm='l2'`) przeprowadzana jest normalizacja L2, która zwraca wektor o długości 1, poprzez podzielenie nieznormalizowanego wektora cech v przez normę L2:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "Aby upewnić się, że rozumiemy mechanizm działania klasy `TfidfTransformer`, przyjrzyjmy się przykładowi i wyliczmy wartość tf-idf wyrazu „dziś” w trzecim dokumencie.\n",
    "\n",
    "Wyraz „dziś” ma w trzecim dokumencie częstość termu równą 2 (tf = 2), jego częstość występowania w tekście również wynosi 3, ponieważ znajduje się we wszystkich trzech zdaniach (df = 3). Możemy więc wyliczyć parametr idf następująco:\n",
    "\n",
    "$$\\text{idf}(\"dziś\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "\n",
    "Teraz w celu obliczenia wartości tf-idf wystarczy dodać 1 do odwrotnej częstości w dokumencie i pomnożyć wynik przez częstość termu:\n",
    "\n",
    "$$\\text{tf-idf}(\"dziś\",d3)= 2 \\times (0+1) = 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print('wartość tf-idf termu \"dziś\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po przeprowadzeniu analogicznych obliczeń dla pozostałych termów w trzecim dokumencie uzyskamy następujący wektor tf-idf: [\n",
    "1,69, 1,69, 2  , 1,29, 3,39, 1,29, 1,29, 1,29, 1,29, 1,29]. Widać jednak, że wartości w tym wektorze cech różnią się od wyników uzyskanych za pomocą klasy `TfidfTransformer`. Musimy jeszcze tylko przeprowadzić normalizację L2, którą wykonamy w sposób zaprezentowany poniżej:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tfi-df}_{norm} = \\frac{[1.69, 1.69, 2.0, 1.29, 3.39, 1.29, 1.29 , 1.29, 1.29, 1.29]}{\\sqrt{[1.69^2, 1.69^2, 2.0^2, 1.29^2, 3.39^2, 1.29^2, 1.29^2 , 1.29^2, 1.29^2, 1.29^2]}}$$\n",
    "\n",
    "$$=[0.3, 0.3, 0.36, 0.23, 0.61, 0.23, 0.23, 0.23, 0.23, 0.23]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"dziś\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, uzyskane wyniki są zgodne z rezultatami zwracanymi przez klasę `TfidfTransformer` (poniżej). Skoro już wiemy, jak są wyliczane wartości tf-idf, możemy przejść do dalszej części rozdziału i wykorzystać omówione koncepcje w odniesieniu do naszego zestawu recenzji filmów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oczyszczanie danych tekstowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'Recenzja'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'Recenzja'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>To :) jest :( test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Recenzja'] = df['Recenzja'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie tekstu na znaczniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sebastian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
    "if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie modelu regresji logistycznej w celu klasyfikowania tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwa znaczniki HTML i znaki interpunkcyjne w celu przyspieszenia przeszukiwania siatki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'Recenzja'].values\n",
    "y_train = df.loc[:25000, 'Sentyment'].values\n",
    "X_test = df.loc[25000:, 'Recenzja'].values\n",
    "y_test = df.loc[25000:, 'Sentyment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważna uwaga na temat parametru `n_jobs`**\n",
    "\n",
    "Bardzo zalecane jest użycie parametru `n_jobs=-1` (zamiast `n_jobs=1`) w powyższym listingu aby wykorzystać wszystkie dostępne rdzenie procesora i przyśpieszyć przeszukiwanie siatki. Jednak część użytkowników systemu Windows zgłaszała problemy z działaniem kodu przy wyznaczonym parametrze `n_jobs=-1` co ma związek z wielordzeniowym konserwowaniem funkcji `tokenizer` i `tokenizer_porter` w systemie Windows. Można ten problem obejść zastępując funkcje  `[tokenizer, tokenizer_porter]` funkcją `[str.split]`. W konsekwencji jednak nie będziemy mogli korzystać z rdzeniowania wyrazów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważna uwaga na temat czasu przetwarzania kodu**\n",
    "\n",
    "Realizacja poniższej komórki **może zająć od 30 do 60 minut** w zależności od konfiguracji sprzętowej, ponieważ zgodnie ze zdefiniowanymi parametrami siatki istnieje 2*2*2*3*5 + 2*2*2*3*5 = 240 modeli do wytrenowania.\n",
    "\n",
    "Jeżeli nie chcesz czekać tak długo, możesz zmniejszyć rozmiar zestawu danych ograniczając liczbę przykładów uczących, np. w następujący sposób:\n",
    "\n",
    "    X_train = df.loc[:2500, 'Recenzja'].values\n",
    "    y_train = df.loc[:2500, 'Sentyment'].values\n",
    "    \n",
    "Pamiętaj jednak, że tak ograniczony rozmiar zestawu uczącego będzie oznaczał kiepską skuteczność modeli. Możesz ewentualnie usunąć parametry z powyższej siatki, aby zmniejszyć liczbę modeli - np. tak, jak pokazano poniżej:\n",
    "\n",
    "    param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                   'vect__stop_words': [stop, None],\n",
    "                   'vect__tokenizer': [tokenizer],\n",
    "                   'clf__penalty': ['l1', 'l2'],\n",
    "                   'clf__C': [1.0, 10.0]},\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "/Users/sebastian/miniconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 41.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acc...\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc89ae1ea0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc89ae1f28>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fbc89ae1ea0>} \n",
      "CV Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "print('Zestaw najlepszych parametrów: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('Dokładność sprawdzianu krzyżowego: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Dokładność testu: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Początek komentarza:\n",
    "    \n",
    "Zwróć uwagę, że `gs_lr_tfidf.best_score_` daje uśredniony wynik k-krotnego sprawdzianu krzyżowego. Np. gdybyśmy korzystali z obiektu `GridSearchCV` składającego się z pięciokrotnej kroswalidacji (tak jak w powyższym przykładzie), atrybut `best_score_` będzie zwracał uśredniony wynik po wyznaczeniu najlepszego modelu za pomocą pięciokrotnej walidacji krzyżowej. Wyjaśnię to na przykładzie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4, 0.2, 0.6, 0.2, 0.4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
    "\n",
    "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
    "    \n",
    "lr = LogisticRegression(random_state=123, multi_class='ovr', solver='lbfgs')\n",
    "cross_val_score(lr, X, y, cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po uruchomieniu powyższego kodu stworzyliśmy prosty zestaw danych składający się z losowych liczb całkowitych, które będą symbolizować etykiety klas. Następnie przekazaliśmy indeksy podzbiorów pięciokrotnego sprawdzianu krzyżowego (`cv3_idx`) funkcji zliczającej `cross_val_score`, która zwróciła pięć wyników dokładności - jest to pięć wartości wyliczonych dla poszczególnych podzbiorów testowych.\n",
    "\n",
    "Użyjmy teraz obiektu `GridSearchCV` i przekażmy mu te same pięć podzbiorów pięciokrotnej kroswalidacji (poprzez indeksy `cv3_idx`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.400, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.200, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.600, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.200, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.400, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)\n",
    "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, wyniki dla pięciu podzbiorów są identyczne, jak wyliczone wcześniej przez funkcję `cross_val_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atrybut `best_score_` (dostępny po wytrenowaniu klasyfikatora) obiektu `GridSearchCV` zwraca uśredniony wynik dokładności dla najlepszego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36000000000000004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik ten jest, jak widać, zgodny z uśrednionym wynikiem dokładności wyliczonym za pomocą funkcji `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36000000000000004"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)\n",
    "cross_val_score(lr, X, y, cv=cv5_idx).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Koniec komentarza.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca z większą ilością danych — algorytmy sieciowe i uczenie pozardzeniowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komórka ta nie jest opisana w książce, lecz\n",
    "# została dodana dla Twojej wygody, żebyć mógł\n",
    "# ropocząć realizowanie kodu od tego miejsca, bez\n",
    "# konieczności uruchamiania wcześniejszych komórek.\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "\n",
    "if not os.path.isfile('movie_data.csv'):\n",
    "    if not os.path.isfile('movie_data.csv.gz'):\n",
    "        print('Umieść tu kopię archiwum movie_data.csv.gz'\n",
    "              'Uzyskasz do niej dostęp'\n",
    "              'a) realizując kod na początku tego notatnika'\n",
    "              'lub b) pobierając go z serwisu GitHub:'\n",
    "              'https://github.com/rasbt/python-machine-learning-'\n",
    "              'book-2nd-edition/blob/master/code/ch08/movie_data.csv.gz')\n",
    "    else:\n",
    "        with gzip.open('movie_data.csv.gz', 'rb') as in_f, \\\n",
    "                open('movie_data.csv', 'wb') as out_f:\n",
    "            out_f.write(in_f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Funkcja `stop` została zdefiniowana wcześniej w tym rozdziale.\n",
    "# Umieściliśmy ją tu dla Twojej wygody, dzięki czemu niniejszy podrozdział\n",
    "# może być traktowany niezależnie od pozostałych fragmentów notatnika w\n",
    "# tym katalogu\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # pomija nagłówek\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "\n",
    "\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:20\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Dokładność: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie tematyczne za pomocą alokacji ukrytej zmiennej Dirichleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozkładanie dokumentów tekstowych za pomocą analizy LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza LDA w bibliotece scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['Recenzja'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Temat %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie pięciu najważniejszych wyrazów dla każdego tematu możemy zgadywać, że model LDA wykrył następujące tematy:\n",
    "    \n",
    "1. Generalnie kiepskie filmy (w rzeczywistości nie jest to kategoria).\n",
    "2. Filmy o rodzinie.\n",
    "3.\tFilmy wojenne.\n",
    "4.\tFilmy artystyczne.\n",
    "5.\tKryminały.\n",
    "6.\tHorrory.\n",
    "7.\tKomedie.\n",
    "8.\tSeriale lub filmy powiązane z serialami.\n",
    "9.\tAdaptacje książek.\n",
    "10.\tFilmy sensacyjne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby upewnić się, że kategorie tworzone na podstawie recenzji mają sens, sprawdźmy trzy filmy oznaczone jako horrory (kategoria szósta, indeks 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror numer #%d:' % (iter_idx + 1))\n",
    "    print(df['Recenzja'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pomocą powyższego kodu wyświetliliśmy pierwszych 300 znaków z recenzji trzech pierw-szych horrorów i widzimy, że recenzje te — mimo że nie znamy tytułów filmów — wyglądają na opisy horrorów (chociaż w przypadku horroru numer 2 równie dobrze moglibyśmy zaklasy-fikować go kategorii 1.: zasadniczo kiepskie filmy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Czytelnicy mogą zignorować poniższą komórkę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ch08.ipynb to script\n",
      "[NbConvertApp] Writing 24745 bytes to ch08.py\n"
     ]
    }
   ],
   "source": [
    "! python ../.convert_notebook_to_script.py --input r08.ipynb --output r08.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
